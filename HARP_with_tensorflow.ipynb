{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HARP_with_tensorflow",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZilingLin/HARP/blob/master/HARP_with_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciEbaKWv6_x2",
        "colab_type": "text"
      },
      "source": [
        "#using the mathod of fake-weight to improve ANRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSWu52JBAn6w",
        "colab_type": "text"
      },
      "source": [
        "##import google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWhtWUg0_9WX",
        "colab_type": "code",
        "outputId": "7e24a316-58c2-4b42-8239-d0b2ac80bd87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/'My Drive'/graduate_work\n",
        "%ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/graduate_work\n",
            "config.py  evaluation.py  main.py   node2vec.py   README.md         utils.py\n",
            "\u001b[0m\u001b[01;34membed\u001b[0m/     \u001b[01;34mgraph\u001b[0m/         model.py  \u001b[01;34m__pycache__\u001b[0m/  requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_tYoy6X638m",
        "colab_type": "text"
      },
      "source": [
        "## import tensorflow and other bags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6buvfiam5UkA",
        "colab_type": "code",
        "outputId": "3eeb4441-bb02-4b6e-a1ae-b92404fb4225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import warnings\n",
        "import collections\n",
        "import scipy.io as scio\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import *\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VWXY-PPEfVT",
        "colab_type": "text"
      },
      "source": [
        "###needed in harp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyLMrk5GEkYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5u0KkZd8gB2",
        "colab_type": "text"
      },
      "source": [
        "##Flags and tf envriomment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8kLo30P8fP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.app.flags.DEFINE_string('datasets', 'citeseer', 'datasets descriptions')\n",
        "tf.app.flags.DEFINE_string('inputEdgeFile', 'graph/citeseer.edgelist', 'input graph edge file')\n",
        "tf.app.flags.DEFINE_string('inputFeatureFile', 'graph/citeseer.feature', 'input graph feature file')\n",
        "tf.app.flags.DEFINE_string('inputLabelFile', 'graph/citeseer.label', 'input graph label file')\n",
        "tf.app.flags.DEFINE_string('outputEmbedFile', 'embed/citeseer.embed', 'output embedding result')\n",
        "tf.app.flags.DEFINE_integer('dimensions', 128, 'embedding dimensions')\n",
        "tf.app.flags.DEFINE_integer('feaDims', 3703, 'feature dimensions')\n",
        "tf.app.flags.DEFINE_integer('walk_length', 80, 'walk length')\n",
        "tf.app.flags.DEFINE_integer('num_walks', 10, 'number of walks')\n",
        "tf.app.flags.DEFINE_integer('window_size', 10, 'window size')\n",
        "tf.app.flags.DEFINE_float('p', 1.0, 'p value')\n",
        "tf.app.flags.DEFINE_float('q', 1.0, 'q value')\n",
        "tf.app.flags.DEFINE_boolean('weighted', False, 'weighted edges')\n",
        "tf.app.flags.DEFINE_boolean('directed', False, 'undirected edges')\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "config_tf = tf.ConfigProto()\n",
        "config_tf.gpu_options.allow_growth = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSuCwW1PDXOn",
        "colab_type": "text"
      },
      "source": [
        "add something that i dont know what it do but required by colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbiuqqHnDkNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.app.flags.DEFINE_string('f', '', 'kernel')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PySNihxa8Gq-",
        "colab_type": "text"
      },
      "source": [
        "##configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAfRo7W08JLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config(object):\n",
        "    def __init__(self):\n",
        "        # hyperparameter\n",
        "        self.struct = [None, 1000, 500, None]\n",
        "        self.alpha = 10\n",
        "        self.reg = 4\n",
        "\n",
        "        # parameters for training\n",
        "        self.batch_size = 512\n",
        "        self.num_sampled = 10\n",
        "        self.max_iters_finaltrain = 20000\n",
        "        self.max_iters_pretrain = 8000\n",
        "        self.learning_rate = 1e-4\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZV_0qHZ9XBf",
        "colab_type": "text"
      },
      "source": [
        "##utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRqWTQ0B9e0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def read_graph(FLAGS, edgeFile):\n",
        "    print('loading graph...')\n",
        "\n",
        "    if FLAGS.weighted:\n",
        "        G = nx.read_edgelist(edgeFile, nodetype=int, data=(('weight', float),), create_using=nx.DiGraph())\n",
        "    else:\n",
        "        G = nx.read_edgelist(edgeFile, nodetype=int, create_using=nx.DiGraph())\n",
        "        for edge in G.edges():\n",
        "            G[edge[0]][edge[1]]['weight'] = 1\n",
        "            # G.add_edge(edge[1], edge[0])\n",
        "            # G[edge[1]][edge[0]]['weight'] = 1\n",
        "\n",
        "    if not FLAGS.directed:\n",
        "        G = G.to_undirected()\n",
        "\n",
        "    return G\n",
        "\n",
        "def read_feature_mat(inputFileName):\n",
        "    data = scio.loadmat(inputFileName)\n",
        "    return data['feature']\n",
        "\n",
        "\n",
        "\n",
        "def read_feature(inputFileName):\n",
        "    f = open(inputFileName, 'r')\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "\n",
        "    features = []\n",
        "    for line in lines[1:]:\n",
        "        l = line.strip('\\n\\r').split(' ')\n",
        "        features.append(l)\n",
        "    features = np.array(features, dtype=np.float32)\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def write_embedding(embedding_result, outputFileName):\n",
        "    f = open(outputFileName, 'w')\n",
        "    N, dims = embedding_result.shape\n",
        "\n",
        "    for i in range(N):\n",
        "        s = ''\n",
        "        for j in range(dims):\n",
        "            if j == 0:\n",
        "                s = str(i) + ',' + str(embedding_result[i, j])\n",
        "            else:\n",
        "                s = s + ',' + str(embedding_result[i, j])\n",
        "        f.writelines(s + '\\n')\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def write_pairs(pairs, outputFileName):\n",
        "    f = open(outputFileName, 'w')\n",
        "    length = len(pairs)\n",
        "\n",
        "    # test\n",
        "    for i in range(length):\n",
        "        s = str(pairs[i, 0]) + ' ' + str(pairs[i, 1])\n",
        "        f.writelines(s + '\\n')\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def read_pairs(inputFileName):\n",
        "    f = open(inputFileName, 'r')\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "\n",
        "    pairlist = []\n",
        "    for line in lines:\n",
        "        l = line.strip('\\n\\r').split(' ')\n",
        "        pair = [int(l[0]), int(l[1])]\n",
        "        pairlist.append(pair)\n",
        "    return pairlist\n",
        "\n",
        "\n",
        "def write_weights(nx_G, outputfile):\n",
        "    f = open(outputfile, 'w')\n",
        "    nodes = nx_G.nodes()\n",
        "    for node in nodes:\n",
        "      neighbors = list(nx_G.neighbors(node))\n",
        "      neighbors_length = len(neighbors)\n",
        "      if neighbors_length != 0:\n",
        "        for n in neighbors:\n",
        "          s = str(node) + ' ' + str(n) + ' ' + str(nx_G[node][n]['weight'])\n",
        "          f.writelines(s + '\\n')\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jDAFi6795qN",
        "colab_type": "text"
      },
      "source": [
        "##evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rByA80G699Nk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def read_label(inputFileName):\n",
        "    f = open(inputFileName, 'r')\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "    N = len(lines)\n",
        "    y = np.zeros(N, dtype=int)\n",
        "    for line in lines:\n",
        "        l = line.strip('\\n\\r').split(' ')\n",
        "        y[int(l[0])] = int(l[1])\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def multiclass_node_classification_eval(X, y, ratio=0.2, rnd=2018):\n",
        "    warnings.filterwarnings('ignore')\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ratio, random_state=rnd)\n",
        "    clf = LinearSVC()\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
        "\n",
        "    return macro_f1, micro_f1\n",
        "\n",
        "\n",
        "def link_prediction_ROC(inputFileName, Embeddings):\n",
        "    f = open(inputFileName, 'r')\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "\n",
        "    X_test = []\n",
        "\n",
        "    for line in lines:\n",
        "        l = line.strip('\\n\\r').split(' ')\n",
        "        X_test.append([int(l[0]), int(l[1]), int(l[2])])\n",
        "\n",
        "    y_true = [X_test[i][2] for i in range(len(X_test))]\n",
        "    y_predict = [\n",
        "        cosine_similarity(Embeddings[X_test[i][0], :].reshape(1, -1), Embeddings[X_test[i][1], :].reshape(1, -1))[0, 0]\n",
        "        for i in range(len(X_test))]\n",
        "    auc = roc_auc_score(y_true, y_predict)\n",
        "\n",
        "    if auc < 0.5:\n",
        "        auc = 1 - auc\n",
        "\n",
        "    return auc\n",
        "\n",
        "\n",
        "def node_classification_F1(Embeddings, y, ratio):\n",
        "    macro_f1_avg = 0\n",
        "    micro_f1_avg = 0\n",
        "    for i in range(10):\n",
        "        rnd = np.random.randint(2018)\n",
        "        macro_f1, micro_f1 = multiclass_node_classification_eval(Embeddings, y, ratio, rnd)\n",
        "        macro_f1_avg += macro_f1\n",
        "        micro_f1_avg += micro_f1\n",
        "    macro_f1_avg /= 10\n",
        "    micro_f1_avg /= 10\n",
        "    print('Macro_f1 average value: ' + str(macro_f1_avg))\n",
        "    print('Micro_f1 average value: ' + str(micro_f1_avg))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcG8dIuo9k6H",
        "colab_type": "text"
      },
      "source": [
        "##node2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsi2M_sm9rq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Graph():\n",
        "    def __init__(self, nx_G, is_directed, p, q):\n",
        "        self.G = nx_G\n",
        "        self.is_directed = is_directed\n",
        "        self.p = p\n",
        "        self.q = q\n",
        "\n",
        "    def node2vec_walk(self, walk_length, start_node):\n",
        "        G = self.G\n",
        "        alias_nodes = self.alias_nodes\n",
        "        alias_edges = self.alias_edges\n",
        "\n",
        "        walk = [start_node]\n",
        "\n",
        "        while len(walk) < walk_length:\n",
        "            cur = walk[-1]\n",
        "            cur_nbrs = sorted(G.neighbors(cur))\n",
        "            if len(cur_nbrs) > 0:\n",
        "                if len(walk) == 1:\n",
        "                    walk.append(\n",
        "                        cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
        "                else:\n",
        "                    prev = walk[-2]\n",
        "                    next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0],\n",
        "                                               alias_edges[(prev, cur)][1])]\n",
        "                    walk.append(next)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return walk\n",
        "\n",
        "    def simulate_walks(self, num_walks, walk_length):\n",
        "        '''\n",
        "        Repeatedly simulate random walks from each node.\n",
        "        '''\n",
        "        G = self.G\n",
        "        walks = []\n",
        "        nodes = list(G.nodes())\n",
        "        print('simulating random walk...')\n",
        "        for walk_iter in range(num_walks):\n",
        "            # print(str(walk_iter + 1), '/', str(num_walks))\n",
        "            random.shuffle(nodes)\n",
        "            for node in nodes:\n",
        "                walks.append(self.node2vec_walk(\n",
        "                    walk_length=walk_length, start_node=node))\n",
        "        return walks\n",
        "\n",
        "    def get_alias_edge(self, src, dst):\n",
        "        '''\n",
        "        Get the alias edge setup lists for a given edge.\n",
        "        '''\n",
        "        G = self.G\n",
        "        p = self.p\n",
        "        q = self.q\n",
        "\n",
        "        # src是上一结点，dst是当前结点，dst_nbr是下一可能的结点\n",
        "        unnormalized_probs = []\n",
        "        for dst_nbr in sorted(G.neighbors(dst)):\n",
        "            if dst_nbr == src:\n",
        "                unnormalized_probs.append(G[dst][dst_nbr]['weight'] / p)\n",
        "            elif G.has_edge(dst_nbr, src):\n",
        "                unnormalized_probs.append(G[dst][dst_nbr]['weight'])\n",
        "            else:\n",
        "                unnormalized_probs.append(G[dst][dst_nbr]['weight'] / q)\n",
        "        norm_const = sum(unnormalized_probs)\n",
        "        normalized_probs = [\n",
        "            float(u_prob) / norm_const for u_prob in unnormalized_probs]\n",
        "\n",
        "        return alias_setup(normalized_probs)\n",
        "\n",
        "    def preprocess_transition_probs(self):\n",
        "        '''\n",
        "        Preprocessing of transition probabilities for guiding the random walks.\n",
        "        '''\n",
        "        G = self.G\n",
        "        is_directed = self.is_directed\n",
        "\n",
        "        alias_nodes = {}\n",
        "        for node in G.nodes():\n",
        "            unnormalized_probs = [G[node][nbr]['weight']\n",
        "                                  for nbr in sorted(G.neighbors(node))]\n",
        "            norm_const = sum(unnormalized_probs)\n",
        "            normalized_probs = [\n",
        "                float(u_prob) / norm_const for u_prob in unnormalized_probs]\n",
        "            alias_nodes[node] = alias_setup(normalized_probs)\n",
        "\n",
        "        alias_edges = {}\n",
        "        if is_directed:\n",
        "            for edge in G.edges():\n",
        "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
        "        else:\n",
        "            for edge in G.edges():\n",
        "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
        "                alias_edges[(edge[1], edge[0])] = self.get_alias_edge(\n",
        "                    edge[1], edge[0])\n",
        "\n",
        "        self.alias_nodes = alias_nodes\n",
        "        self.alias_edges = alias_edges\n",
        "\n",
        "\n",
        "def alias_setup(probs):\n",
        "    '''\n",
        "    Compute utility lists for non-uniform sampling from discrete distributions.\n",
        "    Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
        "    for details\n",
        "    '''\n",
        "    K = len(probs)\n",
        "    q = np.zeros(K)\n",
        "    J = np.zeros(K, dtype=np.int)\n",
        "\n",
        "    smaller = []\n",
        "    larger = []\n",
        "    for kk, prob in enumerate(probs):\n",
        "        q[kk] = K * prob\n",
        "        if q[kk] < 1.0:\n",
        "            smaller.append(kk)\n",
        "        else:\n",
        "            larger.append(kk)\n",
        "\n",
        "    while len(smaller) > 0 and len(larger) > 0:\n",
        "        small = smaller.pop()\n",
        "        large = larger.pop()\n",
        "\n",
        "        J[small] = large\n",
        "        q[large] = q[large] + q[small] - 1.0\n",
        "        if q[large] < 1.0:\n",
        "            smaller.append(large)\n",
        "        else:\n",
        "            larger.append(large)\n",
        "\n",
        "    return J, q\n",
        "\n",
        "\n",
        "def alias_draw(J, q):\n",
        "    '''\n",
        "    Draw sample from a non-uniform discrete distribution using alias sampling.\n",
        "    '''\n",
        "    K = len(J)\n",
        "\n",
        "    kk = int(np.floor(np.random.rand() * K))\n",
        "    if np.random.rand() < q[kk]:\n",
        "        return kk\n",
        "    else:\n",
        "        return J[kk]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsqzjBdTShsf",
        "colab_type": "text"
      },
      "source": [
        "##harp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXyZRAVWsNnd",
        "colab_type": "text"
      },
      "source": [
        "###bfs\n",
        "broad first search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_98dBD6QxeFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bfs(G, cur_node):\n",
        "    component = []\n",
        "    q = deque()\n",
        "    q.append(cur_node)\n",
        "    G.nodes[cur_node]['visit'] = True\n",
        "\n",
        "    while len(q) > 0:\n",
        "        head = q.popleft()\n",
        "        component.append(head)\n",
        "        for nbr in list(G.neighbors(head)):\n",
        "            if G.nodes[nbr]['visit'] == False:\n",
        "                G.nodes[nbr]['visit'] = True\n",
        "                q.append(nbr)\n",
        "    return component"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MQYV0z3sQX3",
        "colab_type": "text"
      },
      "source": [
        "###check if it is connected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_0Kk4E9sWYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_connected(G):\n",
        "    nodes = list(G.nodes())\n",
        "    for node in nodes:\n",
        "        G.nodes[node]['visit'] = False\n",
        "    component = bfs(G, nodes[0])\n",
        "    if len(component) == len(nodes):\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRTAfsZc3xFg",
        "colab_type": "text"
      },
      "source": [
        "###Group Collapsing\n",
        "Every member of the group is permitted to bring their friends to the group, but each of the friends should pass the exam of the group which is the enough vote from the member of the group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZNpcEK73wJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def group_collapsing(G):\n",
        "#     groups = []\n",
        "#     nodes = list(G.nodes())\n",
        "#     for node in nodes:\n",
        "#         G.nodes[node]['visit'] = False\n",
        "\n",
        "#     for node in nodes:\n",
        "#         if G.nodes[node]['visit'] == False:\n",
        "#             group = [node]\n",
        "#             candidates = set(G.neighbors(node))\n",
        "#             G.nodes[node]['visit'] = True\n",
        "#             while len(candidates) > 0:\n",
        "#                 candidate = candidates.pop()\n",
        "#                 if G.nodes[candidate]['visit'] == False:\n",
        "#                     vote = 1\n",
        "#                     for member in group:\n",
        "#                         if candidate in G.neighbors(member):\n",
        "#                             vote += 1\n",
        "#                     if vote > len(group) / 2:\n",
        "#                         group.append(candidate)\n",
        "#                         candidates = candidates.union(set(G.neighbors(candidate)))\n",
        "#                         candidates = candidates.difference(set(group))\n",
        "#                         G.nodes[candidate]['visit'] = True\n",
        "#         groups.append(group)\n",
        "\n",
        "#     return groups"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGe_K13TAcgr",
        "colab_type": "text"
      },
      "source": [
        "###edge collapsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qzrp1cXAXZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edge_collapsing(G):\n",
        "    groups = []\n",
        "    nodes = np.random.permutation(list(G.nodes()))\n",
        "    for node in nodes:\n",
        "        G.nodes[node]['visit'] = False\n",
        "    \n",
        "    for node in nodes:\n",
        "        if G.nodes[node]['visit'] == False:\n",
        "            group = []\n",
        "            for nbr in np.random.permutation(list(G.neighbors(node))):\n",
        "                if G.nodes[nbr]['visit'] == False:\n",
        "                    G.nodes[nbr]['visit'] = True\n",
        "                    group.append(nbr)\n",
        "                    break\n",
        "            G.nodes[node]['visit'] = True\n",
        "            group.append(node)\n",
        "            groups.append(group)\n",
        "\n",
        "    return groups"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HabXlxwT0NQS",
        "colab_type": "text"
      },
      "source": [
        "###star collapsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2n6V_4x0Qrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def star_collapsing(G):\n",
        "    groups = []\n",
        "    nodes = np.random.permutation(list(G.nodes()))\n",
        "    for node in nodes:\n",
        "        G.nodes[node]['visit'] = False\n",
        "    \n",
        "    for node in nodes:\n",
        "        neighbors = np.random.permutation(list(G.neighbors(node)))\n",
        "        group = []\n",
        "        for neighbor in neighbors:\n",
        "            if G.nodes[neighbor]['visit'] == False:\n",
        "                group.append(neighbor)\n",
        "                G.nodes[neighbor]['visit'] = True\n",
        "                if len(group) == 2:\n",
        "                    groups.append(group)\n",
        "                    group = []\n",
        "        if len(group) == 1:\n",
        "            groups.append(group)\n",
        "            group = []\n",
        "\n",
        "    return groups"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvwMX-bhKIsz",
        "colab_type": "text"
      },
      "source": [
        "###build recursive graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJDc2JbbMor3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_regraph(G, groups):\n",
        "    regraph = nx.Graph()\n",
        "    # print(groups)\n",
        "    for i, group in enumerate(groups):\n",
        "        group.sort()\n",
        "        # print(i, len(groups))\n",
        "        for member in group:\n",
        "            G.nodes[member]['collapsing'] = i\n",
        "    \n",
        "    edges = list(G.edges())\n",
        "    for edge in edges:\n",
        "        if G.nodes[edge[0]]['collapsing'] != G.nodes[edge[1]]['collapsing']:\n",
        "            # print(G.nodes[edge[0]]['collapsing'], G.nodes[edge[1]]['collapsing'])\n",
        "            regraph.add_edge(G.nodes[edge[0]]['collapsing'], G.nodes[edge[1]]['collapsing'])\n",
        "    # print('len of the groups', len(groups))\n",
        "    # print('origin:', is_connected(G))\n",
        "    # print('after build regraph', is_connected(regraph))\n",
        "    \n",
        "    # if graph is unweighted\n",
        "    edges = list(regraph.edges())\n",
        "    for edge in edges:\n",
        "        regraph[edge[0]][edge[1]]['weight'] = 1\n",
        "    \n",
        "    nodes = list(regraph.nodes())\n",
        "    nodes.sort(reverse=True)\n",
        "    for node in nodes:\n",
        "        regraph.nodes[node]['visit'] = False\n",
        "\n",
        "    # print('nodes', nodes)\n",
        "    # print('len of nodes', len(regraph.nodes()))\n",
        "    # print('next')\n",
        "\n",
        "    return regraph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D0oL4BQWMpY",
        "colab_type": "text"
      },
      "source": [
        "###prolongate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brrsz5PnWTP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prolongate(G, embedding_input, dimension):\n",
        "    embedding = np.zeros([G.number_of_nodes(), dimension], dtype = np.float32)\n",
        "    # print(np.shape(embedding_input), np.shape(embedding), dimension)\n",
        "    nodes = list(G.nodes())\n",
        "    for node in nodes:\n",
        "        # print(node, G.nodes[node]['collapsing'])\n",
        "        embedding[node] = embedding_input[G.nodes[node]['collapsing']]\n",
        "    return embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8xs-HasC5np",
        "colab_type": "text"
      },
      "source": [
        "###only skipgram model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO8yA2M8DAJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Skipgram_Model:\n",
        "    def __init__(self, config, N, dims, embedding_input = None):\n",
        "        tf.reset_default_graph() \n",
        "        self.config = config\n",
        "        self.N = N\n",
        "        self.dims = dims\n",
        "\n",
        "        self.labels = tf.placeholder(tf.int32, shape=[None, 1])\n",
        "        self.inputs = tf.placeholder(tf.int32, shape=[None])\n",
        "\n",
        "        # Create the embedding variable (each row represent a word embedding vector)\n",
        "        if embedding_input is None:\n",
        "            self.embedding = tf.get_variable('embedding', [self.N, self.dims], initializer=tf.contrib.layers.xavier_initializer())\n",
        "        else:\n",
        "            self.embedding = tf.Variable(embedding_input)\n",
        "\n",
        "        # Lookup the corresponding embedding vectors for each sample in X\n",
        "        self.Y = tf.nn.embedding_lookup(self.embedding, self.inputs)\n",
        "\n",
        "        # for test\n",
        "        # self.test = tf.reduce_sum(tf.add_n([self.Y]))\n",
        "\n",
        "        ############ define variables for skipgram  ####################\n",
        "        # construct variables for nce loss\n",
        "        self.nce_weights = tf.get_variable('nce_weights', [self.N, self.dims],\n",
        "                                           initializer=tf.contrib.layers.xavier_initializer())\n",
        "        self.nce_biases = tf.get_variable('nce_biases', [self.N], initializer=tf.zeros_initializer())\n",
        "\n",
        "        self.loss_sg = self.make_skipgram_loss()\n",
        "\n",
        "        # compute gradients for skipgram\n",
        "        self.train_opt_sg = tf.train.AdamOptimizer(self.config.learning_rate).minimize(self.loss_sg)\n",
        "\n",
        "    def make_skipgram_loss(self):\n",
        "        loss = tf.reduce_sum(tf.nn.sampled_softmax_loss(\n",
        "            weights=self.nce_weights,\n",
        "            biases=self.nce_biases,\n",
        "            labels=self.labels,\n",
        "            inputs=self.Y,\n",
        "            num_sampled=self.config.num_sampled,\n",
        "            num_classes=self.N))\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm87zJ9HPPcA",
        "colab_type": "text"
      },
      "source": [
        "####train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjFiWXcomeqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(G, FLAGS, max_iters, embedding_input = None):\n",
        "    print ('training graph with %d nodes and %d edges' % \n",
        "           (G.number_of_nodes(), G.number_of_edges()))\n",
        "    node2vec_G = Graph(G, FLAGS.directed, FLAGS.p, FLAGS.q)\n",
        "    node2vec_G.preprocess_transition_probs()\n",
        "    walks = node2vec_G.simulate_walks(FLAGS.num_walks, FLAGS.walk_length)\n",
        "\n",
        "    nodes = list(G.nodes())\n",
        "    N = len(nodes)\n",
        "    dims = FLAGS.dimensions\n",
        "    config = Config()\n",
        "    my_context_iter = my_graph_context_batch_iter(walks, FLAGS, config.batch_size)\n",
        "    model = Skipgram_Model(config, N, dims, embedding_input = embedding_input)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "    # sess = tf.Session(config=config_tf)\n",
        "    # sess.run(init)\n",
        "    with tf.Session(config=config_tf) as sess:\n",
        "\n",
        "        # Run the initializer\n",
        "        sess.run(init)\n",
        "\n",
        "        batch_size = config.batch_size\n",
        "        idx = 0\n",
        "        print_every_k_iterations = 1000\n",
        "        start = time.time()\n",
        "        loss_sg = 0\n",
        "\n",
        "        for iter_cnt in range(max_iters):\n",
        "            idx += 1\n",
        "\n",
        "            # train for skip-gram model\n",
        "            batch_index, batch_labels = next(my_context_iter)\n",
        "            feed_dict = {model.inputs: batch_index, model.labels: batch_labels}\n",
        "            _, loss_sg_value = sess.run([model.train_opt_sg, model.loss_sg], feed_dict=feed_dict)\n",
        "            loss_sg += loss_sg_value\n",
        "\n",
        "            if idx % print_every_k_iterations == 0:\n",
        "                end = time.time()\n",
        "                print('iterations: %d' % idx + ', time elapsed: %.2f, ' % (end - start), end='')\n",
        "                print('loss: %.2f, ' % (loss_sg / idx))\n",
        "\n",
        "        print('optimization finished...')\n",
        "        embedding_result = sess.run(model.embedding)\n",
        "    #     print(np.shape(embedding_result))\n",
        "    # print(np.shape(embedding_result))\n",
        "    return embedding_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylLMBQ-9PxTg",
        "colab_type": "text"
      },
      "source": [
        "###graph_coarsening"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCUyq8z5uqtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def graph_coarsening(original_graph):\n",
        "    recursive_graphs = []\n",
        "    recursive_graphs.append(original_graph)\n",
        "    while recursive_graphs[-1].number_of_nodes() > 100:\n",
        "        if len(recursive_graphs) % 2 == 1:\n",
        "            groups = star_collapsing(recursive_graphs[-1])\n",
        "            graph = build_regraph(recursive_graphs[-1], groups)\n",
        "        else:\n",
        "            groups = edge_collapsing(recursive_graphs[-1])\n",
        "            graph = build_regraph(recursive_graphs[-1], groups)\n",
        "        recursive_graphs.append(graph)\n",
        "        print('len of recursive_graphs and size', len(recursive_graphs), recursive_graphs[-1].number_of_nodes())\n",
        "    return recursive_graphs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QiLHQIsP1-o",
        "colab_type": "text"
      },
      "source": [
        "###build_subgraphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-mgdf8QxtEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_subgraphs(G):\n",
        "    subgraphs = []\n",
        "    \n",
        "    nodes = list(G.nodes())\n",
        "    for node in nodes:\n",
        "        G.nodes[node]['visit'] = False\n",
        "\n",
        "    components = []\n",
        "    smallone = []\n",
        "    for node in nodes:\n",
        "        if G.nodes[node]['visit'] == False:\n",
        "            component = bfs(G, node)\n",
        "            component.sort()\n",
        "            # print(len(component), component)\n",
        "            if len(component) > 100:\n",
        "                components.append(component)\n",
        "            else:\n",
        "                smallone = smallone + component\n",
        "    smallone.sort()\n",
        "    components.append(smallone)\n",
        "    \n",
        "    for i, component in enumerate(components):\n",
        "        for j, node in enumerate(component):\n",
        "            G.nodes[node]['subgraph'] = i\n",
        "            G.nodes[node]['at'] = j\n",
        "\n",
        "        # building a new graph to contain the subgraph\n",
        "        subgraph = nx.Graph()\n",
        "        for node in component:\n",
        "            for nbr in G.neighbors(node):\n",
        "                # set weights in new graph, ATTENTION: ONLY if graph is unweighted\n",
        "                weight = 1\n",
        "                subgraph.add_edge(G.nodes[node]['at'], G.nodes[nbr]['at'], weight=weight)\n",
        "        subgraphs.append(subgraph)\n",
        "\n",
        "    return subgraphs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm0EVerR-jTc",
        "colab_type": "text"
      },
      "source": [
        "###generate skipgram training batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6pBYc025SAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recursion_find(left, right, target, length):\n",
        "    if left + 1 >= right:\n",
        "            # print('left', left)\n",
        "            return left, target - length[left]\n",
        "\n",
        "    mid = (left + right) // 2\n",
        "    if target < length[mid]:\n",
        "        return recursion_find(left, mid, target, length)\n",
        "    if target >= length[mid]:\n",
        "        return recursion_find(mid, right, target, length)\n",
        "\n",
        "def my_graph_context_batch_iter(paths, FLAGS, batch_size):\n",
        "    walk_length = FLAGS.walk_length\n",
        "    window_size = FLAGS.window_size\n",
        "    if (walk_length < window_size):\n",
        "        print('ERROR: walk_length %d is less than window_size %d' % (walk_length, window_size))\n",
        "    pairs_in_path = walk_length * window_size * 2 - window_size * (window_size + 1)\n",
        "    total_pairs = pairs_in_path * len(paths)\n",
        "    # print('total_pairs', total_pairs)\n",
        "\n",
        "    length = np.zeros(walk_length, dtype=int)\n",
        "    for i in range(walk_length):\n",
        "        if (i < window_size):\n",
        "            length[i] = window_size + i\n",
        "        elif (i + window_size + 1 >= walk_length):\n",
        "            length[i] = window_size + walk_length - 1 - i\n",
        "        else:\n",
        "            length[i] = window_size * 2\n",
        "    # print('length', length)\n",
        "\n",
        "    total_length = [0]\n",
        "    for i in range(1, walk_length):\n",
        "        total_length.append(total_length[i - 1] + length[i - 1])\n",
        "    # print('total_length', total_length)\n",
        "\n",
        "    while True:\n",
        "        batch = np.zeros(batch_size, dtype=np.int32)\n",
        "        labels = np.zeros((batch_size, 1), dtype=np.int32)\n",
        "        for i in range(batch_size):\n",
        "            idx = np.random.randint(0, total_pairs)\n",
        "            path = int(idx // pairs_in_path)\n",
        "            idx = int(idx % pairs_in_path)\n",
        "            # print('idx', idx)\n",
        "            a, b = recursion_find(0, walk_length, idx, total_length)\n",
        "            k = -1\n",
        "            for j in range(a - window_size, a + window_size + 1):\n",
        "                if j >= 0 and j < walk_length and j != a:\n",
        "                    k += 1\n",
        "                    if k == b:\n",
        "                        b = j\n",
        "                        break\n",
        "            # print('a, b,', a, b)\n",
        "            # print(type(i), type(path), type(a), type(b))\n",
        "            batch[i] = paths[path][a]\n",
        "            labels[i, 0] = paths[path][b]\n",
        "        yield batch, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KummNEr-Vrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_graph_context_all_pairs(path, window_size):\n",
        "    # generating graph context pairs\n",
        "    all_pairs = []\n",
        "    for k in range(len(path)):\n",
        "        for i in range(len(path[k])):\n",
        "            for j in range(i - window_size, i + window_size + 1):\n",
        "                if i == j or j < 0 or j >= len(path[k]):\n",
        "                    continue\n",
        "                else:\n",
        "                    all_pairs.append([path[k][i], path[k][j]])\n",
        "\n",
        "    return np.array(all_pairs, dtype=np.int32)\n",
        "\n",
        "\n",
        "def graph_context_batch_iter(all_pairs, batch_size):\n",
        "    while True:\n",
        "        start_idx = np.random.randint(0, len(all_pairs) - batch_size)\n",
        "        batch_idx = np.array(range(start_idx, start_idx + batch_size))\n",
        "        batch_idx = np.random.permutation(batch_idx)\n",
        "        batch = np.zeros(batch_size, dtype=np.int32)\n",
        "        labels = np.zeros((batch_size, 1), dtype=np.int32)\n",
        "        batch[:] = all_pairs[batch_idx, 0]\n",
        "        labels[:, 0] = all_pairs[batch_idx, 1]\n",
        "        yield batch, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cq7kx9s-2u0",
        "colab_type": "text"
      },
      "source": [
        "###constructing target neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzEGMJsf-8HY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_traget_neighbors(nx_G, X, FLAGS, mode='WAN'):\n",
        "    # construct target neighbor feature matrix\n",
        "    X_target = np.zeros(X.shape)\n",
        "    nodes = nx_G.nodes()\n",
        "\n",
        "    if mode == 'OWN':\n",
        "        # autoencoder for reconstructing itself\n",
        "        return X\n",
        "    elif mode == 'EMN':\n",
        "        # autoencoder for reconstructing Elementwise Median Neighbor\n",
        "        for node in nodes:\n",
        "            neighbors = list(nx_G.neighbors(node))\n",
        "            if len(neighbors) == 0:\n",
        "                X_target[node] = X[node]\n",
        "            else:\n",
        "                temp = np.array(X[node])\n",
        "                for n in neighbors:\n",
        "                    if FLAGS.weighted:\n",
        "                        # weighted sum\n",
        "                        temp = np.vstack((temp, X[n] * nx_G[node][n]['weight']))\n",
        "                    else:\n",
        "                        temp = np.vstack((temp, X[n]))\n",
        "                temp = np.median(temp, axis=0)\n",
        "                X_target[node] = temp\n",
        "        return X_target\n",
        "    elif mode == 'WAN':\n",
        "        # autoencoder for reconstructing Weighted Average Neighbor\n",
        "        for node in nodes:\n",
        "            # put myself ( maybe can be deleted, because second-order will count itself)\n",
        "            temp = np.array(X[node])\n",
        "            # a = temp\n",
        "            # total = len(nodes)\n",
        "            # weights = np.array(1)\n",
        "\n",
        "            # compute neighbors\n",
        "            # alpha = 0.5\n",
        "            # # beta = 0\n",
        "            # first_order = 1\n",
        "            neighbors = list(nx_G.neighbors(node))\n",
        "            neighbors_length = len(neighbors)\n",
        "            if neighbors_length != 0:\n",
        "                for n in neighbors:\n",
        "                    if FLAGS.weighted:\n",
        "                        # weighted sum\n",
        "                        temp = np.vstack((temp, X[n] * nx_G[node][n]['weight']))\n",
        "                    else:\n",
        "                        temp = np.vstack((temp, X[n]))\n",
        "                        # set_n = set(nx_G.successors(n))\n",
        "                        # set_n = set_n.intersection(set(neighbors))\n",
        "                        # second_order = len(set_n) / neighbors_length\n",
        "                        # # weights = np.append(weights, first_order)\n",
        "                        # w = alpha * first_order + (1 - alpha) * second_order\n",
        "                        # nx_G[node][n]['weight'] = w\n",
        "                        # b = np.array(X[n])\n",
        "                        # w = np.dot(a, b)\n",
        "                        # nx_G[node][n]['weight'] = total / (w + 1)\n",
        "                temp = np.mean(temp, axis=0)\n",
        "                # temp = np.average(temp, axis=0, weights = weights)\n",
        "                X_target[node] = temp\n",
        "        return X_target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8n18vkJ-Ryw",
        "colab_type": "text"
      },
      "source": [
        "##main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b6yTixl_ClE",
        "colab_type": "text"
      },
      "source": [
        "###prepare for final train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZqQN1plxJvD",
        "colab_type": "code",
        "outputId": "0f8aab5a-001e-45d3-ec3e-ee45e4a02510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "FLAGS = tf.app.flags.FLAGS\n",
        "inputEdgeFile = FLAGS.inputEdgeFile\n",
        "inputFeatureFile = FLAGS.inputFeatureFile\n",
        "inputLabelFile = FLAGS.inputLabelFile\n",
        "outputEmbedFile = FLAGS.outputEmbedFile\n",
        "window_size = FLAGS.window_size\n",
        "config = Config()\n",
        "\n",
        "G = read_graph(FLAGS, inputEdgeFile)\n",
        "print(G.number_of_nodes(), G.number_of_edges())\n",
        "nodes = G.nodes()\n",
        "# Total number nodes\n",
        "N = len(nodes)\n",
        "dims = FLAGS.dimensions"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading graph...\n",
            "3312 4660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ufyg38NkTTu",
        "colab_type": "code",
        "outputId": "13b937f1-d9b8-4388-c6c2-f3a2c0d1220e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "subgraphs = build_subgraphs(G)\n",
        "embeddings = []\n",
        "for s, subgraph in enumerate(subgraphs):\n",
        "    print ('Subgraph %d with %d nodes and %d edges' % \n",
        "           (s, subgraph.number_of_nodes(), subgraph.number_of_edges()))\n",
        "    if not is_connected(subgraph):\n",
        "        print(\"graph picies\")\n",
        "        embedding = train(subgraph, FLAGS, config.max_iters_pretrain)\n",
        "    else:\n",
        "        print ('Graph Coarsening...')\n",
        "        recursive_graphs = graph_coarsening(subgraph)\n",
        "        embedding_input = None\n",
        "        for i in range(len(recursive_graphs) - 1, -1, -1):\n",
        "            # temp = list(recursive_graphs[i].nodes())\n",
        "            # temp.sort(reverse=True)\n",
        "            # print(temp)\n",
        "            # print(len(recursive_graphs[i].nodes()))\n",
        "            print('recursive_graph level', i)\n",
        "            embedding = train(recursive_graphs[i], FLAGS, config.max_iters_pretrain, embedding_input)\n",
        "            if i > 0:\n",
        "                embedding_input = prolongate(recursive_graphs[i - 1], embedding, FLAGS.dimensions)\n",
        "    embeddings.append(embedding)\n",
        "final_embedding = np.zeros([N, FLAGS.dimensions], dtype = np.float32)\n",
        "for node in nodes:\n",
        "    s = G.nodes[node]['subgraph']\n",
        "    a = G.nodes[node]['at']\n",
        "    final_embedding[node] = embeddings[s][a]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Subgraph 0 with 2110 nodes and 3720 edges\n",
            "Graph Coarsening...\n",
            "len of recursive_graphs and size 2 1407\n",
            "len of recursive_graphs and size 3 868\n",
            "len of recursive_graphs and size 4 578\n",
            "len of recursive_graphs and size 5 365\n",
            "len of recursive_graphs and size 6 238\n",
            "len of recursive_graphs and size 7 152\n",
            "len of recursive_graphs and size 8 96\n",
            "recursive_graph level 7\n",
            "training graph with 96 nodes and 342 edges\n",
            "simulating random walk...\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "iterations: 1000, time elapsed: 11.28, loss: 1211.42, \n",
            "iterations: 2000, time elapsed: 21.01, loss: 1176.86, \n",
            "iterations: 3000, time elapsed: 30.74, loss: 1138.55, \n",
            "iterations: 4000, time elapsed: 40.57, loss: 1109.85, \n",
            "iterations: 5000, time elapsed: 50.34, loss: 1087.66, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OOT1jnp5a0n",
        "colab_type": "text"
      },
      "source": [
        "### final train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaUwPnOj_H5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# target neighbors must be constructed before graph context pairs\n",
        "print('constructing traget neighbors...')\n",
        "start_time = time.time()\n",
        "X_target = construct_traget_neighbors(G, X, FLAGS, mode='WAN')  # 对所有结点，重建邻近目标结点\n",
        "end_time = time.time()\n",
        "print('time consumed for constructing traget neighbors: %.2f' % (end_time - start_time))\n",
        "\n",
        "# Perform random walks to generate graph context\n",
        "node2vec_G = Graph(G, FLAGS.directed, FLAGS.p, FLAGS.q)\n",
        "node2vec_G.preprocess_transition_probs()\n",
        "walks = node2vec_G.simulate_walks(FLAGS.num_walks, FLAGS.walk_length)\n",
        "\n",
        "print('generating graph context pairs...')\n",
        "start_time = time.time()\n",
        "all_pairs = generate_graph_context_all_pairs(walks, window_size)  # 游走序列中窗口内的结点对 加入all_pairs\n",
        "end_time = time.time()\n",
        "print('time consumed for constructing graph context: %.2f' % (end_time - start_time))\n",
        "\n",
        "config = Config()\n",
        "config.struct[0] = FLAGS.feaDims\n",
        "config.struct[-1] = FLAGS.dimensions\n",
        "\n",
        "model = Model(config, N, dims, X_target, final_embedding) # attension\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANIo9zmu5Y5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Final train')\n",
        "batch_size = config.batch_size\n",
        "max_iters = config.max_iters_finaltrain\n",
        "idx = 0\n",
        "print_every_k_iterations = 1000\n",
        "start = time.time()\n",
        "loss = 0\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "# with tf.Session(config=config_tf) as sess:\n",
        "sess = tf.Session(config=config_tf)\n",
        "sess.run(init)\n",
        "for iter_cnt in range(max_iters):\n",
        "    idx += 1\n",
        "\n",
        "    # train for skip-gram model\n",
        "    batch_index, batch_labels = next(graph_context_batch_iter(all_pairs, batch_size))\n",
        "\n",
        "    feed_dict = {model.inputs: batch_index, model.labels: batch_labels}\n",
        "    _, loss_value = sess.run([model.train_opt, model.loss], feed_dict=feed_dict)\n",
        "    loss += loss_value\n",
        "\n",
        "    if idx % print_every_k_iterations == 0:\n",
        "        end = time.time()\n",
        "        print('iterations: %d' % idx + ', time elapsed: %.2f, ' % (end - start), end='')\n",
        "        print('loss: %.2f, ' % (loss / idx), end='')\n",
        "\n",
        "        y = read_label(inputLabelFile)\n",
        "        embedding_result = sess.run(model.embedding)\n",
        "        macro_f1, micro_f1 = multiclass_node_classification_eval(embedding_result, y, 0.7)\n",
        "        print('[macro_f1 = %.4f, micro_f1 = %.4f]' % (macro_f1, micro_f1))\n",
        "embedding_result = sess.run(model.embedding)\n",
        "\n",
        "print('optimization finished...')\n",
        "y = read_label(inputLabelFile)\n",
        "print('repeat 10 times for node classification with random split...')\n",
        "node_classification_F1(embedding_result, y, 0.95)\n",
        "# print('saving embedding result...')\n",
        "# write_embedding(embedding_result, outputEmbedFile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aS_WmwIuGY9",
        "colab_type": "text"
      },
      "source": [
        "###scoring and output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5v8kpBSuQZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "node_classification_F1(embedding_result, y, 0.7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rswI5JfndhjE",
        "colab_type": "text"
      },
      "source": [
        "###extra train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZgCC2N9dn0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('extra train')\n",
        "for iter_cnt in range(max_iters):\n",
        "    idx += 1\n",
        "\n",
        "    # train for skip-gram model\n",
        "    batch_index, batch_labels = next(graph_context_batch_iter(all_pairs, batch_size))\n",
        "\n",
        "    feed_dict = {model.inputs: batch_index, model.labels: batch_labels}\n",
        "    _, loss_value = sess.run([model.train_opt, model.loss], feed_dict=feed_dict)\n",
        "    loss += loss_value\n",
        "\n",
        "    if idx % print_every_k_iterations == 0:\n",
        "        end = time.time()\n",
        "        print('iterations: %d' % idx + ', time elapsed: %.2f, ' % (end - start), end='')\n",
        "        print('loss: %.2f, ' % (loss / idx), end='')\n",
        "\n",
        "        y = read_label(inputLabelFile)\n",
        "        embedding_result = sess.run(model.embedding)\n",
        "        macro_f1, micro_f1 = multiclass_node_classification_eval(embedding_result, y, 0.7)\n",
        "        print('[macro_f1 = %.4f, micro_f1 = %.4f]' % (macro_f1, micro_f1))\n",
        "embedding_result = sess.run(model.embedding)\n",
        "print('optimization finished...')\n",
        "y = read_label(inputLabelFile)\n",
        "print('repeat 10 times for node classification with random split...')\n",
        "node_classification_F1(embedding_result, y, 0.70)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWe02XNBdqgn",
        "colab_type": "text"
      },
      "source": [
        "###train only for network structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS0EJhP-INlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Train for skipgram model only')\n",
        "embedding = train(G, FLAGS, config.max_iters_finaltrain, final_embedding)\n",
        "y = read_label(inputLabelFile)\n",
        "print('repeat 10 times for node classification with random split...')\n",
        "node_classification_F1(embedding, y, 0.95)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHwhI0GGEar3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Train for skipgram model only with no harp')\n",
        "embedding = train(G, FLAGS, config.max_iters_finaltrain)\n",
        "y = read_label(inputLabelFile)\n",
        "print('repeat 10 times for node classification with random split...')\n",
        "node_classification_F1(embedding, y, 0.95)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMWSuRwsFUuY",
        "colab_type": "text"
      },
      "source": [
        "##the end"
      ]
    }
  ]
}